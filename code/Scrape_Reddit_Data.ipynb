{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPpklanH0Fr2",
        "outputId": "34f18eaf-6aad-45f6-ecfd-b752bd220b12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.12.14)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update_checker, prawcore, praw\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install praw"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDWkO1Agiqmb",
        "outputId": "4a34ef12-3105-4d95-ac50-cf92ec2ebaac"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import praw\n",
        "import datetime\n",
        "import re\n",
        "import contractions"
      ],
      "metadata": {
        "id": "65mMxCVr0Pnj"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "eHgwGPEq0WS6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_new_posts(reddit, subreddits, limit=5):\n",
        "  all_posts = {}\n",
        "  for subreddit_name in subreddits:\n",
        "    all_posts[subreddit_name] = []\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "    print(f\"Fetching posts from r/{subreddit_name}...\")\n",
        "    posts = subreddit.new(limit=limit) # Fetch the latest posts\n",
        "    count = 0\n",
        "    for submission in posts:\n",
        "      if submission is not None and submission.author is not None:\n",
        "        temp_post_dict = {\"id\": submission.id, \"url\":submission.url, \"created_utc\":submission.created_utc, \"author\":submission.author.name,  \"title\":submission.title, \"post\":submission.selftext, \"score\":submission.score, \"upvote_ratio\":submission.upvote_ratio, \"num_comments\":submission.num_comments}\n",
        "        count = count+1\n",
        "        all_posts[subreddit_name].append(temp_post_dict)\n",
        "    print(count)\n",
        "    time.sleep(1)\n",
        "  return all_posts\n",
        "\n",
        "def save_json(data):\n",
        "  for key in data.keys():\n",
        "    with open(\"/content/{0}.json\".format(key), 'w', encoding='utf-8') as json_file:\n",
        "      json.dump(data[key], json_file, indent=4, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "SppZB2k10ey6"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json(file_path):\n",
        "  with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "  return data\n",
        "\n",
        "def preprocess_text(text):\n",
        "  text = text.lower() # Convert to lowercase\n",
        "  text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) # Remove URLs\n",
        "  text = re.sub(r'\\S*@\\S*\\s?',' ',text) # Remove emails\n",
        "  text = re.sub(r'@\\S+', ' ', text) # Remove mentions if any\n",
        "  text = contractions.fix(text) # Expand contractions like don't to do not\n",
        "  text = re.sub(r'\\W', ' ', text) # Allow only word characters\n",
        "  text = re.sub(r'\\s+', ' ', text).strip() # Remove extra spaces\n",
        "  return text\n",
        "\n",
        "def prepare_data_for_analysis(json_file, city=\"default\"):\n",
        "  data = load_json(json_file)\n",
        "  results = []\n",
        "  # for posts in data.items():\n",
        "  for post in data:\n",
        "    title = preprocess_text(post.get(\"title\", \"\"))\n",
        "    content = preprocess_text(post.get(\"post\", \"\"))\n",
        "    combined_text = f\"{title} {content}\"\n",
        "    results.append({\n",
        "        \"city\": city,\n",
        "        \"id\": post[\"id\"],\n",
        "        \"url\": post[\"url\"],\n",
        "        \"created_utc\": post[\"created_utc\"],\n",
        "        \"author\": post[\"author\"],\n",
        "        \"text\": combined_text,\n",
        "        \"reddit_score\": post[\"score\"],\n",
        "        \"reddit_upvote_ratio\": post[\"upvote_ratio\"]})\n",
        "  return results\n",
        "\n",
        "def save_to_json(data, output_file):\n",
        "  with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(data, f, indent=4, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "3f9L_oufuHXw"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_posts_via_id(reddit, ids):\n",
        "  all_posts = {}\n",
        "  count = 0\n",
        "  for id in ids:\n",
        "    submission = reddit.submission(id=id)\n",
        "    subreddit_name = submission.subreddit.display_name\n",
        "    if subreddit_name not in all_posts.keys():\n",
        "      all_posts[subreddit_name] = []\n",
        "    if submission is not None and submission.author is not None:\n",
        "      temp_post_dict = {\"id\": submission.id, \"url\":submission.url, \"created_utc\":submission.created_utc, \"author\":submission.author.name,  \"title\":submission.title, \"post\":submission.selftext, \"score\":submission.score, \"upvote_ratio\":submission.upvote_ratio, \"num_comments\":submission.num_comments}\n",
        "      all_posts[subreddit_name].append(temp_post_dict)\n",
        "      count = count + 1\n",
        "      if count%50 == 0: # to limit rate limit\n",
        "        print(count)\n",
        "        time.sleep(1)\n",
        "  return all_posts"
      ],
      "metadata": {
        "id": "pesZyZDDvXo0"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_comments_from_submission(reddit, ids):\n",
        "  all_comments = {}\n",
        "  for id in ids:\n",
        "    submission = reddit.submission(id=id)\n",
        "    subreddit_name = submission.subreddit.display_name\n",
        "    if subreddit_name not in all_comments.keys():\n",
        "      all_comments[subreddit_name] = []\n",
        "    submission.comments.replace_more(limit=None)\n",
        "    count = 0\n",
        "    comment_list = submission.comments.list()\n",
        "    for comment in comment_list:\n",
        "      if comment is not None and comment.author is not None:\n",
        "        temp_comment_dict = {\"id\": comment.id, \"url\": comment.permalink, \"created_utc\": comment.created_utc, \"author\": str(comment.author.name), \"submission_id\": comment.link_id, \"parent_id\": comment.parent_id, \"post\": comment.body, \"score\": comment.score}\n",
        "        all_comments[subreddit_name].append(temp_comment_dict)\n",
        "        count = count + 1\n",
        "        if count%50 == 0: # to limit rate limit\n",
        "          print(count)\n",
        "          time.sleep(1)\n",
        "  return all_comments"
      ],
      "metadata": {
        "id": "adWkaevn1Nfk"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a Reddit instance with credentials\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"kcl92OEqmozGzUpB8JJ5xw\",\n",
        "    client_secret=\"LPQzJlaUx-hyNtQnWHXLMfyKJYalmg\",\n",
        "    user_agent=\"social_media_listening_cities_v1.0\",\n",
        "    check_for_async=False)\n",
        "\n",
        "#subreddits to get the posts from\n",
        "subreddits = [\"bangalore\", \"mumbai\", \"delhi\", \"visakhapatnam\", \"newyorkcity\" ]\n",
        "\n",
        "file_paths = [\"/content/bangalore.json\", \"/content/delhi.json\", \"/content/mumbai.json\", \"/content/newyorkcity.json\", \"/content/visakhapatnam.json\"]\n",
        "cities = [\"bangalore\",\"delhi\",\"mumbai\",\"newyorkcity\",\"visakhapatnam\"]\n"
      ],
      "metadata": {
        "id": "7ay5VtEm1mJo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch new posts from subreddits\n",
        "\n",
        "# all_posts = fetch_new_posts(reddit=reddit, subreddits=subreddits, limit=2000)\n",
        "\n",
        "# Save posts into json files\n",
        "# save_json(all_posts)"
      ],
      "metadata": {
        "id": "gqqgHSHdCHhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use reddit post ids to fetch data and save into json\n",
        "\n",
        "# file_paths = [\"/content/raw_bangalore.json\", \"/content/raw_delhi.json\", \"/content/raw_mumbai.json\", \"/content/raw_newyorkcity.json\", \"/content/raw_visakhapatnam.json\"]\n",
        "\n",
        "# for file in file_paths:\n",
        "#   df = pd.read_json(file)\n",
        "#   ids = df['id'].values.tolist()\n",
        "#   all_posts = fetch_posts_via_id(reddit=reddit, ids=ids)\n",
        "#   save_json(all_posts)\n",
        "#   time.sleep(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNb4uJjjuyop",
        "outputId": "fad0aa81-798c-40f7-9817-fba7b8e4ac70"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "250\n",
            "300\n",
            "350\n",
            "400\n",
            "450\n",
            "500\n",
            "550\n",
            "600\n",
            "650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial Preprocess reddit data\n",
        "\n",
        "for i in range(len(file_paths)):\n",
        "  output_file = f\"/content/processed_{cities[i]}.json\"\n",
        "  processed_data = prepare_data_for_analysis(file_paths[i], city=cities[i])\n",
        "  save_to_json(processed_data, output_file)"
      ],
      "metadata": {
        "id": "JnN-ec0U33Sg"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load reddit comments using post ids\n",
        "\n",
        "for file in file_paths:\n",
        "  df = pd.read_json(file)\n",
        "  ids = df['id'].values.tolist()\n",
        "  all_comments = fetch_comments_from_submission(reddit=reddit, ids=ids)\n",
        "  save_json(all_comments)\n",
        "  print(f\"{file} completed\")\n",
        "  time.sleep(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JozdNsbvCB_b",
        "outputId": "2db65a5b-6898-4206-cb7d-4d98d61ec025"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "100\n",
            "150\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "/content/visakhapatnam.json completed\n"
          ]
        }
      ]
    }
  ]
}